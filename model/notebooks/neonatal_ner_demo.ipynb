{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "947bbf16",
   "metadata": {},
   "source": [
    "# Clinical NLP System for Neonatal Discharge Summary Entity Extraction\n",
    "\n",
    "This notebook demonstrates a comprehensive clinical NLP system designed to extract entities from neonatal discharge summaries.\n",
    "\n",
    "## Target Entities\n",
    "- **P_ID**: Patient identifier\n",
    "- **Gestational_Age**: Age at birth in weeks\n",
    "- **Sex**: Gender (Male/Female)\n",
    "- **Birth_Weight**: Weight at birth\n",
    "- **Diagnosis**: Medical diagnosis\n",
    "- **Treatment_Respiratory**: Respiratory treatment information\n",
    "- **Treatment_Medication**: Medication treatment details\n",
    "- **Outcome**: Patient outcome/discharge status\n",
    "\n",
    "## Output Formats\n",
    "1. JSON structured format with each entity as a key-value pair\n",
    "2. Markdown table format with one row\n",
    "3. Evaluation metrics: Precision, Recall, and F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0a4585",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import essential libraries for NLP processing, evaluation, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec3a9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the path to import our modules\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# NLP libraries\n",
    "try:\n",
    "    import spacy\n",
    "    print(\"spaCy loaded successfully\")\n",
    "except ImportError:\n",
    "    print(\"spaCy not installed. Install with: pip install spacy\")\n",
    "\n",
    "# Evaluation libraries\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Our custom modules\n",
    "try:\n",
    "    from neonatal_ner import NeonatalNER\n",
    "    from entity_patterns import EntityPatterns\n",
    "    from evaluation import EntityEvaluator\n",
    "    print(\"Custom modules loaded successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"Could not import custom modules: {e}\")\n",
    "    print(\"Make sure you're running this from the notebooks directory\")\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c7f4a2",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Sample Data\n",
    "\n",
    "Load sample neonatal discharge summary sentences and their ground truth annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25449154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample annotations\n",
    "with open('../data/sample_annotations.json', 'r') as f:\n",
    "    sample_data = json.load(f)\n",
    "\n",
    "# Extract texts and annotations\n",
    "sample_texts = [item['text'] for item in sample_data]\n",
    "sample_annotations = [item['annotations'] for item in sample_data]\n",
    "\n",
    "print(f\"Loaded {len(sample_texts)} sample texts\")\n",
    "print(\"\\nFirst sample:\")\n",
    "print(f\"Text: {sample_texts[0]}\")\n",
    "print(f\"Annotations: {json.dumps(sample_annotations[0], indent=2)}\")\n",
    "\n",
    "# Create a DataFrame for easier viewing\n",
    "df_samples = pd.DataFrame(sample_data)\n",
    "print(\"\\nSample data overview:\")\n",
    "print(df_samples.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b16a027",
   "metadata": {},
   "source": [
    "## 3. Define Entity Extraction Classes\n",
    "\n",
    "Initialize our clinical patterns and NER system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61fa228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the entity patterns\n",
    "patterns = EntityPatterns()\n",
    "\n",
    "# Display some example patterns\n",
    "print(\"Sample Patient ID Patterns:\")\n",
    "for i, pattern in enumerate(patterns.patient_id_patterns[:3]):\n",
    "    print(f\"{i+1}. {pattern}\")\n",
    "\n",
    "print(\"\\nSample Gestational Age Patterns:\")\n",
    "for i, pattern in enumerate(patterns.gestational_age_patterns[:3]):\n",
    "    print(f\"{i+1}. {pattern}\")\n",
    "\n",
    "print(\"\\nSample Sex Patterns:\")\n",
    "for i, pattern in enumerate(patterns.sex_patterns[:3]):\n",
    "    print(f\"{i+1}. {pattern}\")\n",
    "\n",
    "print(\"\\nAll pattern categories:\")\n",
    "all_patterns = patterns.get_all_patterns()\n",
    "for category, pattern_list in all_patterns.items():\n",
    "    print(f\"- {category}: {len(pattern_list)} patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f032bba",
   "metadata": {},
   "source": [
    "## 4. Build Rule-Based Entity Extractor\n",
    "\n",
    "Initialize our NeonatalNER system and test it on sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e64886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the NER system\n",
    "ner_system = NeonatalNER()\n",
    "\n",
    "# Test on the first sample\n",
    "test_text = sample_texts[0]\n",
    "test_ground_truth = sample_annotations[0]\n",
    "\n",
    "print(\"Testing on sample text:\")\n",
    "print(f\"Text: {test_text}\")\n",
    "print(\"\\nGround Truth:\")\n",
    "print(json.dumps(test_ground_truth, indent=2))\n",
    "\n",
    "# Extract entities\n",
    "result = ner_system.extract_entities(test_text, test_ground_truth)\n",
    "\n",
    "print(\"\\nExtracted Entities:\")\n",
    "print(json.dumps(result['entities'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4b6933",
   "metadata": {},
   "source": [
    "## 5. Implement spaCy NER Model (Rule-based approach)\n",
    "\n",
    "For demonstration, we'll use our rule-based approach which is more suitable for clinical text patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9278f5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test individual entity extraction methods\n",
    "print(\"Testing individual entity extraction:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "test_text = sample_texts[0]\n",
    "print(f\"Text: {test_text}\")\n",
    "print()\n",
    "\n",
    "# Test each entity extraction\n",
    "entities = ner_system._extract_all_entities(test_text)\n",
    "\n",
    "entity_names = [\n",
    "    'P_ID', 'Gestational_Age', 'Sex', 'Birth_Weight',\n",
    "    'Diagnosis', 'Treatment_Respiratory', 'Treatment_Medication', 'Outcome'\n",
    "]\n",
    "\n",
    "for entity_name in entity_names:\n",
    "    extracted_value = getattr(entities, entity_name)\n",
    "    ground_truth_value = test_ground_truth.get(entity_name)\n",
    "    \n",
    "    status = \"✓\" if extracted_value == ground_truth_value else \"✗\"\n",
    "    \n",
    "    print(f\"{entity_name:20}: {extracted_value or 'null':30} | GT: {ground_truth_value or 'null':30} {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e680a8a4",
   "metadata": {},
   "source": [
    "## 6. Create Output Formatting Functions\n",
    "\n",
    "Demonstrate JSON and Markdown table formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2f9da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract entities and format outputs\n",
    "result = ner_system.extract_entities(test_text, test_ground_truth)\n",
    "\n",
    "print(\"1. JSON FORMAT:\")\n",
    "print(\"=\"*50)\n",
    "print(result['json_output'])\n",
    "\n",
    "print(\"\\n\\n2. MARKDOWN TABLE FORMAT:\")\n",
    "print(\"=\"*50)\n",
    "print(result['markdown_table'])\n",
    "\n",
    "# Display the markdown table in a more readable format\n",
    "print(\"\\n\\n3. FORMATTED MARKDOWN TABLE:\")\n",
    "print(\"=\"*50)\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(result['markdown_table']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab60b18",
   "metadata": {},
   "source": [
    "## 7. Develop Evaluation Metrics Calculator\n",
    "\n",
    "Calculate and display evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b21c393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display evaluation metrics\n",
    "if 'evaluation' in result:\n",
    "    eval_data = result['evaluation']\n",
    "    \n",
    "    print(\"EVALUATION METRICS:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Precision: {eval_data['precision']:.3f} ({eval_data['correct_extractions']}/{eval_data['total_extractions']})\")\n",
    "    print(f\"Recall:    {eval_data['recall']:.3f} ({eval_data['correct_extractions']}/{eval_data['total_actual']})\")\n",
    "    print(f\"F1-Score:  {eval_data['f1_score']:.3f}\")\n",
    "    \n",
    "    print(\"\\nPER-ENTITY RESULTS:\")\n",
    "    print(\"-\" * 50)\n",
    "    for entity, status in eval_data['entity_results'].items():\n",
    "        print(f\"{entity:20}: {status}\")\n",
    "        \n",
    "    # Create a simple visualization\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "    \n",
    "    metrics = ['Precision', 'Recall', 'F1-Score']\n",
    "    values = [eval_data['precision'], eval_data['recall'], eval_data['f1_score']]\n",
    "    \n",
    "    bars = ax.bar(metrics, values, color=['skyblue', 'lightgreen', 'gold'])\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Evaluation Metrics for Single Sample')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{value:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No evaluation data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce940324",
   "metadata": {},
   "source": [
    "## 8. Train Custom NER Model (Batch Processing)\n",
    "\n",
    "Process all samples and evaluate overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb745c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all samples\n",
    "all_results = ner_system.batch_extract(sample_texts, sample_annotations)\n",
    "\n",
    "print(f\"Processed {len(all_results)} samples\")\n",
    "print(\"\\nSample Results:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, result in enumerate(all_results[:3]):  # Show first 3 results\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"Text: {sample_texts[i][:100]}...\")\n",
    "    \n",
    "    if 'evaluation' in result:\n",
    "        eval_data = result['evaluation']\n",
    "        print(f\"Precision: {eval_data['precision']:.3f}, Recall: {eval_data['recall']:.3f}, F1: {eval_data['f1_score']:.3f}\")\n",
    "    \n",
    "    print(\"Extracted entities:\")\n",
    "    for entity, value in result['entities'].items():\n",
    "        print(f\"  {entity}: {value or 'null'}\")\n",
    "        \n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db2b579",
   "metadata": {},
   "source": [
    "## 9. Test Entity Extraction Pipeline\n",
    "\n",
    "Comprehensive pipeline demonstration with a new test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73faa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new test case\n",
    "new_test_text = \"\"\"\n",
    "Patient ID: NB-2024-007. Female infant born at 31 weeks gestational age with birth weight of 1650 grams.\n",
    "Primary diagnosis: Severe respiratory distress syndrome and patent ductus arteriosus.\n",
    "Treatment included mechanical ventilation for 5 days followed by CPAP support.\n",
    "Medications: Surfactant therapy, caffeine citrate for apnea, and indomethacin for PDA closure.\n",
    "Patient showed good improvement and was discharged home after 6 weeks in stable condition.\n",
    "\"\"\"\n",
    "\n",
    "new_ground_truth = {\n",
    "    \"P_ID\": \"NB-2024-007\",\n",
    "    \"Gestational_Age\": \"31 weeks\",\n",
    "    \"Sex\": \"Female\",\n",
    "    \"Birth_Weight\": \"1650 grams\",\n",
    "    \"Diagnosis\": \"Severe respiratory distress syndrome and patent ductus arteriosus\",\n",
    "    \"Treatment_Respiratory\": \"mechanical ventilation for 5 days followed by CPAP support\",\n",
    "    \"Treatment_Medication\": \"Surfactant therapy, caffeine citrate, indomethacin\",\n",
    "    \"Outcome\": \"discharged home after 6 weeks in stable condition\"\n",
    "}\n",
    "\n",
    "print(\"COMPLETE PIPELINE TEST\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Input Text: {new_test_text.strip()}\")\n",
    "print(\"\\nGround Truth:\")\n",
    "print(json.dumps(new_ground_truth, indent=2))\n",
    "\n",
    "# Run the complete pipeline\n",
    "pipeline_result = ner_system.extract_entities(new_test_text, new_ground_truth)\n",
    "\n",
    "# Display results using the built-in pretty printer\n",
    "ner_system.print_results(pipeline_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc3b8f6",
   "metadata": {},
   "source": [
    "## 10. Performance Analysis and Visualization\n",
    "\n",
    "Analyze performance across different entity types and create comprehensive visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8e6e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect evaluation results from all samples\n",
    "evaluator = EntityEvaluator()\n",
    "all_predictions = [result['entities'] for result in all_results]\n",
    "batch_evaluation = evaluator.batch_evaluate(all_predictions, sample_annotations)\n",
    "\n",
    "print(\"BATCH EVALUATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Overall Precision: {batch_evaluation['aggregate_precision']:.3f}\")\n",
    "print(f\"Overall Recall: {batch_evaluation['aggregate_recall']:.3f}\")\n",
    "print(f\"Overall F1-Score: {batch_evaluation['aggregate_f1_score']:.3f}\")\n",
    "print(f\"Total Samples: {batch_evaluation['sample_count']}\")\n",
    "\n",
    "# Per-entity performance\n",
    "per_entity = batch_evaluation['per_entity_metrics']\n",
    "print(\"\\nPER-ENTITY PERFORMANCE:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "entity_data = []\n",
    "for entity, metrics in per_entity.items():\n",
    "    entity_data.append({\n",
    "        'Entity': entity,\n",
    "        'Precision': metrics['precision'],\n",
    "        'Recall': metrics['recall'],\n",
    "        'F1-Score': metrics['f1_score'],\n",
    "        'Correct': metrics['correct'],\n",
    "        'Total_Predicted': metrics['total_predicted'],\n",
    "        'Total_Actual': metrics['total_actual']\n",
    "    })\n",
    "    print(f\"{entity:20}: P={metrics['precision']:.3f}, R={metrics['recall']:.3f}, F1={metrics['f1_score']:.3f}\")\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "df_performance = pd.DataFrame(entity_data)\n",
    "print(\"\\nPerformance DataFrame:\")\n",
    "print(df_performance.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533354b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Overall Metrics Bar Chart\n",
    "overall_metrics = ['Precision', 'Recall', 'F1-Score']\n",
    "overall_values = [\n",
    "    batch_evaluation['aggregate_precision'],\n",
    "    batch_evaluation['aggregate_recall'],\n",
    "    batch_evaluation['aggregate_f1_score']\n",
    "]\n",
    "\n",
    "bars1 = axes[0, 0].bar(overall_metrics, overall_values, color=['skyblue', 'lightgreen', 'gold'])\n",
    "axes[0, 0].set_ylim(0, 1)\n",
    "axes[0, 0].set_title('Overall Performance Metrics')\n",
    "axes[0, 0].set_ylabel('Score')\n",
    "\n",
    "for bar, value in zip(bars1, overall_values):\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                   f'{value:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# 2. Per-Entity F1 Scores\n",
    "entities = df_performance['Entity']\n",
    "f1_scores = df_performance['F1-Score']\n",
    "\n",
    "bars2 = axes[0, 1].bar(range(len(entities)), f1_scores, color='coral')\n",
    "axes[0, 1].set_xticks(range(len(entities)))\n",
    "axes[0, 1].set_xticklabels(entities, rotation=45, ha='right')\n",
    "axes[0, 1].set_ylim(0, 1)\n",
    "axes[0, 1].set_title('F1-Score by Entity Type')\n",
    "axes[0, 1].set_ylabel('F1-Score')\n",
    "\n",
    "for i, (bar, value) in enumerate(zip(bars2, f1_scores)):\n",
    "    axes[0, 1].text(i, value + 0.01, f'{value:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# 3. Precision vs Recall Scatter Plot\n",
    "precision_vals = df_performance['Precision']\n",
    "recall_vals = df_performance['Recall']\n",
    "\n",
    "scatter = axes[1, 0].scatter(precision_vals, recall_vals, c=f1_scores, cmap='viridis', s=100, alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Precision')\n",
    "axes[1, 0].set_ylabel('Recall')\n",
    "axes[1, 0].set_title('Precision vs Recall by Entity')\n",
    "axes[1, 0].set_xlim(0, 1)\n",
    "axes[1, 0].set_ylim(0, 1)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add entity labels to scatter plot\n",
    "for i, entity in enumerate(entities):\n",
    "    axes[1, 0].annotate(entity, (precision_vals.iloc[i], recall_vals.iloc[i]), \n",
    "                       xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "# Add colorbar\n",
    "plt.colorbar(scatter, ax=axes[1, 0], label='F1-Score')\n",
    "\n",
    "# 4. Entity Extraction Success Rate\n",
    "success_rates = df_performance['Correct'] / df_performance['Total_Actual'] * 100\n",
    "success_rates = success_rates.fillna(0)  # Handle division by zero\n",
    "\n",
    "bars4 = axes[1, 1].bar(range(len(entities)), success_rates, color='lightsteelblue')\n",
    "axes[1, 1].set_xticks(range(len(entities)))\n",
    "axes[1, 1].set_xticklabels(entities, rotation=45, ha='right')\n",
    "axes[1, 1].set_ylim(0, 100)\n",
    "axes[1, 1].set_title('Extraction Success Rate by Entity')\n",
    "axes[1, 1].set_ylabel('Success Rate (%)')\n",
    "\n",
    "for i, (bar, value) in enumerate(zip(bars4, success_rates)):\n",
    "    axes[1, 1].text(i, value + 1, f'{value:.0f}%', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSUMMARY STATISTICS:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Best performing entity (F1): {entities[f1_scores.idxmax()]} ({f1_scores.max():.3f})\")\n",
    "print(f\"Worst performing entity (F1): {entities[f1_scores.idxmin()]} ({f1_scores.min():.3f})\")\n",
    "print(f\"Average F1-Score: {f1_scores.mean():.3f}\")\n",
    "print(f\"Standard deviation: {f1_scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da654d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates a comprehensive clinical NLP system for extracting entities from neonatal discharge summaries. The system:\n",
    "\n",
    "1. **Identifies 8 key entities** relevant to neonatal care\n",
    "2. **Provides dual output formats** (JSON and Markdown table)\n",
    "3. **Includes evaluation metrics** (Precision, Recall, F1-score)\n",
    "4. **Uses clinical pattern matching** optimized for medical text\n",
    "5. **Provides detailed performance analysis** across entity types\n",
    "\n",
    "### Key Features:\n",
    "- Rule-based approach suitable for clinical terminology\n",
    "- Comprehensive evaluation framework\n",
    "- Batch processing capabilities\n",
    "- Detailed performance visualization\n",
    "- Easily extensible pattern system\n",
    "\n",
    "### Next Steps:\n",
    "1. Expand training data with more annotated samples\n",
    "2. Implement transformer-based models for comparison\n",
    "3. Add more sophisticated entity linking\n",
    "4. Integrate with clinical workflow systems\n",
    "5. Implement active learning for continuous improvement"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
